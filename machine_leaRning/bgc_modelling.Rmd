---
title: "BGC Abundance Modelling"
author: "Emiliano Pereira"
date: "March 4, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = F)
```

## __Outline__
#### 1. Introduction
#### 2. Linear Regression Models
#### 3. Random Forest Regression Models
#### 4. Random Forest Binary Classification + Regression Models
#### 5. Evaluate Random Forest Binary Classification + Regression Models
#### 6. Random Forest Binary Classification Models

&nbsp;

## __1. Introduction__
##### In this work, we create BGC class-specific models to predict the abundance of BGC classes in metagenomic samples. These models use the class abundance as the response variables, and the domains abundance of the corresponding classes, as the predictor variables. To train and evaluate the models, the data is randomly split (within the functions) to create the training and testing data sets.

&nbsp;

#### __We use four functions:__
##### __1. binary_model_fun__: Trains and tests a binary classification model to predict the presence of BGC classes.
##### __2. regression_model_fun__: Trains and tests a regression model to predict the abundance of BGC classes.
##### __3. double_model_fun__: Integrates the previous two models. It creates a binary model based on the complete data set and a regression model using only the non-zero abundance rows of the response variables. Briefly, this function applies the binary model to identify samples where a class is present, and subsequently applies the regression model on this subset of samples to predict their abundance.
##### __4. iter_model_fun__: This function iteratively runs the double_model_fun to evaluate its performance by randomly generating in each iteration different training and testing data sets.  

&nbsp;

##### Load libraries, functions, and data
```{r results='hide', message=FALSE, warning=FALSE}
library(randomForest)
library(dplyr)
library(tidyr)
library(ggplot2)
library(vegan)
library(miscTools)
library(e1071)
library(ggplot2)
source("toolbox/toolbox.R")

TBL_DOM <- read.table("../data/139prokrich_domain_abund_u-prot_u-orf.csv", header=T, sep=",", row.names = 1)
TBL_CLASS <- read.table("../data/139prokrich_class_abund.csv", header=T, sep=",", row.names = 1)
class2dom <- read.table("../data/class2domain.csv", header=T, sep=",", row.names = 1)

```
##### Transform and join   tables
```{r}

data <- left_join(x = tibble::rownames_to_column( TBL_DOM/rowSums(TBL_DOM) ),
              y = tibble::rownames_to_column(TBL_CLASS/rowSums(TBL_CLASS) ), 
              by = "rowname" )


```
##### Select bgc classes and colors
```{r}
bgc_classes <- c("bacteriocin","ectoine",
                 "nrps","t1pks","t3pks",
                 "terpene")

class2color <- cbind(class = bgc_classes, 
                     color = c("#ECD078","#D95B43","#C02942",
                               "#542437","#53777A","#000000"))


```
&nbsp;

## __2. Linear Regression Models__
##### Run the regression_model_fun using lm
```{R results='hide', message=FALSE, warning=FALSE}
lm_abund <- regression_model_fun(bgc = bgc_classes, 
                                 data = data,
                                 regression_method = "lm",
                                 class2dom = class2dom
                                 )


```

##### Visualize output

```{R}
lapply(lm_abund,"[", 1:3 ) %>% 
       do.call("rbind", .)

x <- lapply(lm_abund,"[[", 5) %>% 
      plyr::ldply(., data.frame)

colnames(x) <- c("class","pred","resp")

ggplot(x, aes(x = resp, y=pred)) +
       geom_abline(intercept = 0, slope = 1, color = "gray60") +  
       geom_point(aes(color = class ), alpha = 0.5) +
       facet_wrap( ~ class, ncol = 2, nrow = 3, scales = "free") +
       scale_color_manual(values = class2color[,"color"]) + 
       xlab("response test") +
       ylab("predicted") +
       expand_limits(y=0) 

```
&nbsp;

### __3. Random Forest Regression Models__
##### Run the regression_model_fun using rf 
```{R}

rf_abund <- regression_model_fun(bgc = bgc_classes, 
                                 data = data,
                                 regression_method = "rf",
                                 class2dom = class2dom
                                 )
```

##### Visualize output

```{R}
lapply(rf_abund,"[", 1:3 ) %>% 
       do.call("rbind", .)

x <- lapply(rf_abund,"[[", 5) %>% 
      plyr::ldply(., data.frame)

colnames(x) <- c("class","pred","resp")

ggplot(x, aes(x = resp, y=pred)) +
       geom_abline(intercept = 0, slope = 1, color = "gray60") +  
       geom_point(aes(color = class ), alpha = 0.5) +
       facet_wrap( ~ class, ncol = 2, nrow = 3, scales = "free") +
        scale_color_manual(values = class2color[,"color"]) + 
       xlab("response test") +
       ylab("predicted") +
       expand_limits(y=0) 

```
&nbsp;

## __4. Random Forest Binary Classification + Regression Models__
##### Run the double_models_fun using rf
```{R}
drf_abund <- double_models_fun(bgc = bgc_classes, 
                              data = data,
                              binary_method = "rf",
                              regression_method = "rf",
                              class2dom = class2dom
                              ) 


```

##### Visualize output

```{R}
c <- lapply(drf_abund,"[", 1:3 ) %>% 
            plyr::ldply(., data.frame)


x <- lapply(drf_abund,"[[", 5) %>% 
            plyr::ldply(., data.frame)

colnames(x) <- c("class","pred","resp")

ggplot(x, aes(x = resp, y=pred)) +
       geom_abline(intercept = 0, slope = 1, color = "gray60") +  
       geom_point( aes(color = class ), alpha = 0.5) +
       facet_wrap( ~ class, ncol = 2, nrow = 3, scales = "free") +
       scale_color_manual(values = class2color[,"color"]) + 
       xlab("response test") +
       ylab("predicted") +
       expand_limits(y=0) 

```
&nbsp;

## __5. Evaluate Random Forest Binary Classification + Regression Models__
#### Run the iter_model_fun with 100 iterations
```{R}
drf_abund_iter <- iter_model_fun(bgc = bgc_classes, 
                                 data = data,
                                 binary_method = "rf",
                                 regression_method = "rf",
                                 iter= 100,
                                 class2dom = class2dom
                                )
```

##### Visualize output: correlation distributions

```{R message=FALSE, warning=FALSE}
f <- function(x) lapply(x, "[[", "cor") %>% do.call("rbind", .) 
cors <- lapply(drf_abund_iter, f ) %>%
               plyr::ldply(., data.frame)

colnames(cors) <- c("class","cor")

ggplot(cors, aes(x = class, y = cor, fill = class)) +
       geom_jitter(alpha = .1) +
       geom_violin( aes(fill = class), alpha = .5, width = 1, scale="count") +
       stat_summary(fun.y = median ,geom='point') +
       xlab("BGC class") +
       ylab("Pearson Correlation Coefficient") +
       scale_fill_manual(values = class2color[,"color"]) +  
       theme_light() +
       theme( axis.text.y =  element_text(size=10, color = "black"), 
              axis.text.x =  element_blank())

```

##### Visualize output: MSE distributions

```{R}
f <- function(x) lapply(x, "[[", "mse") %>% do.call("rbind", .)
mses <- lapply(drf_abund_iter, f ) %>%
               plyr::ldply(., data.frame)

colnames(mses) <- c("class","mse")

ggplot(mses, aes(x = class, y = mse, fill = class)) +
       geom_jitter(alpha = .1) +
       geom_violin( aes(fill = class), alpha = .5, width = 1, scale="count") +
       scale_fill_manual(values = class2color[,"color"]) + 
       xlab("BGC class") +
       ylab("MSE") +
       theme_light() +
       theme( axis.text.y =  element_text(size=10, color = "black"), 
              axis.text.x =  element_blank())


```
&nbsp;


#### Plot MSE vs cor:
```{R}
x <- cbind(mses,cors[,2], 1:100 )
colnames(x) <- c("class","mse","cor","iter")

ggplot(x, aes(x = mse, y=cor, label = iter)) +
       geom_point(alpha = 0.5, aes(color = class)) +
       scale_color_manual(values = class2color[,"color"]) + 
       geom_text(check_overlap = TRUE, size = 4, hjust = 0) +
       facet_wrap( ~ class, ncol = 2, nrow = 3, scales = "free") 

```

#### Visualize all model stats together.
```{R}
lm <- lapply(lm_abund,"[", 1:3 ) %>% 
            plyr::ldply(., data.frame) 

rf <- lapply(rf_abund,"[", 1:3 ) %>% 
             plyr::ldply(., data.frame) 

drf <- lapply(drf_abund,"[", 1:3 ) %>% 
              plyr::ldply(., data.frame) 

l <- length(bgc_classes)
model <- rep( c("lm","rf","drf"), times = c(l,l,l))

m <- rbind(lm, rf, drf ) %>%
     as.data.frame() %>%
     tibble::rownames_to_column() %>%
     cbind(model,.)

colnames(m) <- c("model","id","class","cor","cor.pvalue","mse")
```

#### MSE plots:
```{R}
ggplot(m, aes(x = model, y=mse)) +
       geom_bar(stat = "identity", alpha = 0.5, aes(fill = class )) +
       scale_fill_manual(values = class2color[,"color"]) + 
       facet_wrap( ~ class, ncol = 2, nrow = 3, scales = "free") 

```
#### Correlation plots:

```{R}
ggplot(m, aes(x = model, y=cor)) +
       geom_bar(stat = "identity", alpha = 0.5, aes(fill = class )) +
       scale_fill_manual(values = class2color[,"color"]) + 
       facet_wrap( ~ class, ncol = 2, nrow = 3, scales = "free") 

```


### __6. Random Forest Binary Classification Models__
##### Run binary_model_fun using rf
```{R}
rf_binary <- binary_model_fun(bgc = bgc_classes, 
                              data = data, 
                              binary_method = "rf",
                              class2dom = class2dom
                              )

```

##### Visualize output

```{R}

lapply( rf_binary,"[", 1:3 ) %>% 
       do.call("rbind", .)

```

##### Get importance of predictors: binray model

```{R}
models_classify <- sapply(rf_binary,"[", 6)[c(1:5)]

x <- lapply(X = models_classify, FUN =  importance) %>%
     do.call("rbind", .) %>%
     as.data.frame() 

colnames(x) <- c("MeanDecreaseGini")

class2dom_redu <- class2dom %>% filter(class %in% bgc_classes ) 

bm <-   inner_join( tibble::rownames_to_column(x),
        class2dom_redu,
        by = c("rowname" = "domain"))  %>% 
        arrange(., class, MeanDecreaseGini ) %>%
        mutate(class_and_dom = paste(class, rowname) )

colnames(bm)[1] <- "domain"

bm <- bm %>% filter(class != "ectoine" )
bm$class_and_dom <- factor(bm$class_and_dom, levels = bm$class_and_dom)
bm$domain <- factor(bm$domain, levels = bm$domain)
bm$class <- factor(bm$class, levels = unique(bm$class))


```

##### Get importance of predictors: regression model

```{R}

rm <- rf_abund$terpene$regression_model %>% 
      importance() %>%
      as.data.frame() %>%
      tibble::rownames_to_column() %>%
      arrange(., IncNodePurity )

colnames(rm) <- c("domain","IncMSE","IncNodePurity")
rm$domain <- factor(rm$domain, levels = rm$domain)


```
##### Plot importances: binary model

```{R}
bml <- bm %>% 
       select(domain, MeanDecreaseGini, class ) %>%
       gather(key  = "method", value = "value", MeanDecreaseGini)

order <- -( bml$domain %>%  as.numeric()  )

ggplot(bml, aes(x = reorder(bml$domain, order), y = value )) +
       geom_bar(stat = "identity", aes(fill = class), alpha = 0.7 ) +
       scale_fill_manual(values = class2color[,"color"]) + 
       coord_flip() +
       facet_wrap( ~ method, ncol = 2, nrow = 1, scales = "free") +
       xlab("class and domains") 
```

### Plot importances: regression model (terpene)

```{R}
rml <- rm %>% 
       select(domain, IncMSE, IncNodePurity ) %>%
       gather(key  = "method", value = "value", IncMSE, IncNodePurity)

order <- -( rml$domain %>%  as.numeric()  )

ggplot(rml, aes(x = reorder(rml$domain, order), y = value )) +
       geom_bar(stat = "identity", fill = class2color[6,"color"], alpha = 0.5 ) +
       coord_flip() +
       facet_wrap( ~ method, ncol = 2, nrow = 1, scales = "free") +
       xlab("class and domains")

```
